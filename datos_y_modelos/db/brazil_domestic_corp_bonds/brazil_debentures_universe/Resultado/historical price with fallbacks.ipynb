{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install pyrefdata 0.4.0\n",
    "#%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional, Union\n",
    "# import pandas as pd\n",
    "# import pyrefdata\n",
    "# import bqapi\n",
    "\n",
    "\n",
    "# def format_datestring(date: Union[str, pd.Timestamp]) -> str:\n",
    "#     \"\"\"Formats the date as YYYYMMDD string.\"\"\"\n",
    "#     return pd.Timestamp(date).strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "# def get_historical_px_last(\n",
    "#     securities: List[str],\n",
    "#     start_date: Union[str, pd.Timestamp],\n",
    "#     end_date: Union[str, pd.Timestamp],\n",
    "#     currency: str = \"BRL\",\n",
    "#     frequency: str = \"monthly\",  # 'daily' or 'monthly'\n",
    "#     **kwargs,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Get historical PX_LAST prices for a list of securities using pyrefdata.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     securities : list of str\n",
    "#         Bloomberg tickers, e.g., ['ED345652@ande Corp']\n",
    "#     start_date : str or pd.Timestamp\n",
    "#         Start date, e.g., '2005-03-10'\n",
    "#     end_date : str or pd.Timestamp\n",
    "#         End date, e.g., '2012-01-01'\n",
    "#     currency : str\n",
    "#         Currency override.\n",
    "#     frequency : str\n",
    "#         'daily' or 'monthly'.\n",
    "#     kwargs : dict\n",
    "#         Additional arguments to pass to bqapi.Session\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Date-indexed DataFrame with securities as columns.\n",
    "#     \"\"\"\n",
    "#     start = format_datestring(start_date)\n",
    "#     end = format_datestring(end_date)\n",
    "\n",
    "#     with bqapi.Session(**kwargs) as session:\n",
    "#         df = pyrefdata.get_historical_data(\n",
    "#             session,\n",
    "#             securities,\n",
    "#             fields=[\"PX_LAST\"],\n",
    "#             start_date=start,\n",
    "#             end_date=end,\n",
    "#             currency=currency,\n",
    "#             non_trading_day_fill_option=\"all_calendar_days\",\n",
    "#             override_option=\"close\",\n",
    "#             ignore_errors=True,\n",
    "#         ).reset_index()\n",
    "\n",
    "#     # Pivot the result to get one column per security\n",
    "#     df = df.pivot(index=\"date\", columns=\"Security\", values=\"PX_LAST\").ffill()\n",
    "\n",
    "#     # Ensure the index is datetime before resampling\n",
    "#     df.index = pd.to_datetime(df.index)\n",
    "\n",
    "#     # Monthly resample if requested\n",
    "#     if frequency == \"monthly\":\n",
    "#         df = df.resample(\"M\").last()\n",
    "\n",
    "#     df.index.name = \"DATE\"\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# securities = ['ED345652@ande Corp']\n",
    "\n",
    "\n",
    "\n",
    "# start_date = '2005-03-10'\n",
    "# end_date = '2012-01-01'\n",
    "\n",
    "# df = get_historical_px_last(\n",
    "#     securities,\n",
    "#     start_date=start_date,\n",
    "#     end_date=end_date,\n",
    "#     currency=\"BRL\",\n",
    "#     frequency=\"monthly\"  # or 'daily'\n",
    "# )\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Optional\n",
    "import pandas as pd\n",
    "import pyrefdata\n",
    "import bqapi\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "def format_datestring(date: Union[str, pd.Timestamp]) -> str:\n",
    "    return pd.Timestamp(date).strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def fetch_historical_data_for_sources(\n",
    "    base_ticker: str,\n",
    "    pricing_sources: List[str],\n",
    "    start_date: Union[str, pd.Timestamp],\n",
    "    end_date: Union[str, pd.Timestamp],\n",
    "    currency: str = \"BRL\",\n",
    "    frequency: str = \"monthly\",\n",
    "    fallback_years: int = 10,\n",
    "    **kwargs,\n",
    ") -> Dict[str, Union[Dict[str, pd.DataFrame], str]]:\n",
    "    \"\"\"\n",
    "    Fetch prices from all sources. Evaluates best source with most data. Includes correct NaN handling.\n",
    "    \"\"\"\n",
    "    start_fmt = format_datestring(start_date)\n",
    "    end_fmt = format_datestring(end_date)\n",
    "    all_data: Dict[str, pd.DataFrame] = {}\n",
    "    diagnostics: Dict[str, str] = {}\n",
    "\n",
    "    for source in pricing_sources:\n",
    "        ticker = f\"{base_ticker}@{source.lower()} Corp\"\n",
    "        try:\n",
    "            with bqapi.Session(**kwargs) as session:\n",
    "                df = pyrefdata.get_historical_data(\n",
    "                    session,\n",
    "                    [ticker],\n",
    "                    fields=[\"PX_LAST\"],\n",
    "                    start_date=start_fmt,\n",
    "                    end_date=end_fmt,\n",
    "                    currency=currency,\n",
    "                    non_trading_day_fill_option=\"all_calendar_days\",\n",
    "                    override_option=\"close\",\n",
    "                    ignore_errors=True,\n",
    "                ).reset_index()\n",
    "\n",
    "                if df.empty:\n",
    "                    diagnostics[source] = \"Empty in original window\"\n",
    "                    # Try fallback to recent years\n",
    "                    df = pyrefdata.get_historical_data(\n",
    "                        session,\n",
    "                        [ticker],\n",
    "                        fields=[\"PX_LAST\"],\n",
    "                        start_date=format_datestring(pd.Timestamp.today() - pd.DateOffset(years=fallback_years)),\n",
    "                        end_date=format_datestring(pd.Timestamp.today()),\n",
    "                        currency=currency,\n",
    "                        non_trading_day_fill_option=\"all_calendar_days\",\n",
    "                        override_option=\"close\",\n",
    "                        ignore_errors=True,\n",
    "                    ).reset_index()\n",
    "\n",
    "                    if df.empty:\n",
    "                        diagnostics[source] += \" + fallback failed\"\n",
    "                        continue\n",
    "                    else:\n",
    "                        diagnostics[source] += \" + fallback success\"\n",
    "\n",
    "                df = df.pivot(index=\"date\", columns=\"Security\", values=\"PX_LAST\")\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "\n",
    "                if frequency == \"monthly\":\n",
    "                    df = df.resample(\"M\").last()\n",
    "\n",
    "                # Fill internal gaps only\n",
    "                df = df.ffill()\n",
    "\n",
    "                # Truncate after last real price\n",
    "                last_valid_index = df.last_valid_index()\n",
    "                if last_valid_index is not None:\n",
    "                    df = df.loc[:last_valid_index]\n",
    "\n",
    "                df.index.name = \"DATE\"\n",
    "                all_data[source] = df\n",
    "                diagnostics[source] = diagnostics.get(source, \"Success\")\n",
    "\n",
    "        except Exception as e:\n",
    "            diagnostics[source] = f\"Exception: {str(e)}\"\n",
    "            continue\n",
    "\n",
    "    contribution_counts = {\n",
    "        source: df.notna().sum().sum() for source, df in all_data.items()\n",
    "    }\n",
    "\n",
    "    best_source = (\n",
    "        max(contribution_counts.items(), key=lambda x: x[1])[0]\n",
    "        if contribution_counts else None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"data\": all_data,\n",
    "        \"best_source\": best_source,\n",
    "        \"diagnostics\": diagnostics,\n",
    "    }\n",
    "\n",
    "def build_actual_observation_table(\n",
    "    tickers: List[str],\n",
    "    pricing_sources: List[str],\n",
    "    start_date: Union[str, pd.Timestamp],\n",
    "    end_date: Union[str, pd.Timestamp],\n",
    "    currency: str = \"BRL\",\n",
    "    fallback_years: int = 10,\n",
    "    **kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame:\n",
    "    - Rows: All calendar dates in range [start_date, end_date]\n",
    "    - Columns: <ticker>@<best_source>\n",
    "    - Values: Actual PX_LAST observations only, no forward-fill\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "    diagnostics = {}\n",
    "\n",
    "    # Define full date range index\n",
    "    full_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "\n",
    "        result = fetch_historical_data_for_sources(\n",
    "            ticker,\n",
    "            pricing_sources,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            frequency=\"daily\",  # Get raw data\n",
    "            currency=currency,\n",
    "            fallback_years=fallback_years,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        best_source = result['best_source']\n",
    "        diagnostics[ticker] = best_source\n",
    "\n",
    "        if not best_source:\n",
    "            print(f\"  ❌ No data for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        df = result['data'][best_source].copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.reindex(full_index)  # Align to full date range\n",
    "        df.columns = [f\"{ticker}@{best_source}\"]\n",
    "\n",
    "        combined_df = combined_df.join(df, how=\"outer\") if not combined_df.empty else df\n",
    "\n",
    "    print(\"\\n✅ Pricing Sources Used:\")\n",
    "    for k, v in diagnostics.items():\n",
    "        print(f\"  {k}: {v or '❌ No data'}\")\n",
    "\n",
    "    combined_df.index.name = \"DATE\"\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xls = pd.ExcelFile(\"universo_brazil_deb.xlsx\")\n",
    "# df = xls.parse(sheet_name='Sheet1')\n",
    "# tickers = df[\"Ticker\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"pcs_debentures.xlsx\")\n",
    "df = xls.parse(sheet_name='Sheet1')\n",
    "pricing_sources = df[\"pricing_source\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pricing_sources = ['bval', 'psan', 'brad', 'cmdb', 'anbe', 'ande', 'icor', 'miae', 'xpcp']\n",
    "#pricing_sources = pricing_sources[0:5]\n",
    "#tickers = ['ED345652','TEQU11','MNRV16']\n",
    "#tickers = tickers[0:3]\n",
    "\n",
    "\n",
    "xls = pd.ExcelFile(\"parte_4.xlsx\")\n",
    "df = xls.parse(sheet_name='Sheet1')\n",
    "tickers = df[\"Ticker\"].dropna().unique().tolist()\n",
    "\n",
    "df = build_actual_observation_table(\n",
    "    tickers,\n",
    "    pricing_sources,\n",
    "    start_date=\"1989-10-01\",\n",
    "    end_date=\"2025-08-04\",\n",
    ")\n",
    "   \n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "df.to_excel(\"resultado_parte_4.xlsx\")\n",
    "print(f\"✅ File 4 saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use today's date in format YYYY.MM.DD\n",
    "today = date.today().strftime('%Y.%m.%d')\n",
    "\n",
    "# Set output path\n",
    "home_directory = os.path.expanduser('~')\n",
    "export_filename = f\"{home_directory}/Downloads/waterfallbondpriced-{today}.xlsx\"\n",
    "\n",
    "# Assuming your DataFrame is called df_fixed\n",
    "with pd.ExcelWriter(export_filename, date_format='yyyy/mm/dd',\n",
    "                    datetime_format='yyyy/mm/dd') as writer:\n",
    "    df.to_excel(writer)\n",
    "\n",
    "print(f\"✅ File saved to: {export_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"resultado_parte_6.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quebrar a lista de tickers em 20 pedaços pois é muito grande pra processar de uma vez\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('universo_brazil_deb.xlsx')  \n",
    "\n",
    "coluna = df['Ticker']\n",
    "\n",
    "partes = []\n",
    "tamanho_parte = len(coluna) // 20 + (len(coluna) % 20 > 0)\n",
    "\n",
    "for i in range(0, len(coluna), tamanho_parte):\n",
    "    partes.append(coluna.iloc[i:i+tamanho_parte])\n",
    "\n",
    "for i, parte in enumerate(partes):\n",
    "    parte.to_excel(f'parte_{i+1}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 files. Merging into one sheet…\n",
      "  -> merged resultado_parte_1.xlsx | shape now (13092, 179)\n",
      "  -> merged resultado_parte_2.xlsx | shape now (13092, 413)\n",
      "  -> merged resultado_parte_3.xlsx | shape now (13092, 659)\n",
      "  -> merged resultado_parte_4.xlsx | shape now (13092, 662)\n",
      "  -> merged resultado_parte_5.xlsx | shape now (13092, 952)\n",
      "  -> merged resultado_parte_6.xlsx | shape now (13092, 1243)\n",
      "  -> merged resultado_parte_7.xlsx | shape now (13092, 1544)\n",
      "  -> merged resultado_parte_8.xlsx | shape now (13092, 1692)\n",
      "  -> merged resultado_parte_9.xlsx | shape now (13092, 1789)\n",
      "  -> merged resultado_parte_10.xlsx | shape now (13092, 1892)\n",
      "  -> merged resultado_parte_11.xlsx | shape now (13092, 1974)\n",
      "  -> merged resultado_parte_12.xlsx | shape now (13092, 2075)\n",
      "  -> merged resultado_parte_13.xlsx | shape now (13092, 2175)\n",
      "  -> merged resultado_parte_14.xlsx | shape now (13092, 2314)\n",
      "  -> merged resultado_parte_15.xlsx | shape now (13092, 2564)\n",
      "  -> merged resultado_parte_16.xlsx | shape now (13092, 2824)\n",
      "  -> merged resultado_parte_17.xlsx | shape now (13092, 3122)\n",
      "  -> merged resultado_parte_18.xlsx | shape now (13092, 3421)\n",
      "  -> merged resultado_parte_19.xlsx | shape now (13092, 3687)\n",
      "  -> merged resultado_parte_20.xlsx | shape now (13092, 3952)\n",
      "\n",
      "Final shape: 13092 rows x 3952 ticker columns (+ DATE = 3953 total Excel columns)\n",
      "  -> wrote 5000 rows to Excel...\n",
      "  -> wrote 10000 rows to Excel...\n",
      "✅ Saved: resultado_unico.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, re, gc\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PATTERN = \"resultado_parte_*.xlsx\"   # ou \"parte_*.xlsx\"\n",
    "OUTFILE = \"resultado_unico.xlsx\"\n",
    "DATE_CANDIDATES = [\"DATE\", \"Date\", \"date\", \"DATA\"]\n",
    "EXCEL_MAX_COLS = 16384  # limite do Excel (colunas)\n",
    "PRINT_EVERY_FILE = 1    # print de progresso a cada N arquivos\n",
    "PRINT_EVERY_ROWS = 5000 # print de progresso ao escrever linhas no Excel\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def numeric_key(path):\n",
    "    m = re.search(r'(\\d+)', os.path.basename(path))\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "def read_part(path):\n",
    "    \"\"\"Lê uma parte, normaliza a coluna DATE como índice datetime e converte valores para float32.\"\"\"\n",
    "    df = pd.read_excel(path)\n",
    "    # encontrar/normalizar DATE\n",
    "    date_col = next((c for c in DATE_CANDIDATES if c in df.columns), df.columns[0])\n",
    "    if date_col != \"DATE\":\n",
    "        df = df.rename(columns={date_col: \"DATE\"})\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DATE\"]).set_index(\"DATE\")\n",
    "\n",
    "    # converter colunas não-numéricas para numérico (coerce -> NaN), depois forçar float32\n",
    "    non_num = [c for c in df.columns if not np.issubdtype(df[c].dtype, np.number)]\n",
    "    if non_num:\n",
    "        df[non_num] = df[non_num].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df = df.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# MERGE (UNION DE TICKERS) POR DATE\n",
    "# =========================\n",
    "files = sorted(glob.glob(PATTERN), key=numeric_key)\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo encontrado para o padrão: {PATTERN}\")\n",
    "\n",
    "print(f\"Found {len(files)} files. Merging into one sheet…\")\n",
    "\n",
    "master = None\n",
    "for i, f in enumerate(files, 1):\n",
    "    part = read_part(f)\n",
    "\n",
    "    if master is None:\n",
    "        master = part\n",
    "    else:\n",
    "        # se houver tickers em comum, priorize valores já existentes (non-NaN) no master\n",
    "        overlap = master.columns.intersection(part.columns)\n",
    "        if len(overlap) > 0:\n",
    "            master[overlap] = master[overlap].combine_first(part[overlap])\n",
    "            part = part.drop(columns=overlap)\n",
    "\n",
    "        master = master.join(part, how=\"outer\")\n",
    "\n",
    "    if i % PRINT_EVERY_FILE == 0:\n",
    "        print(f\"  -> merged {os.path.basename(f)} | shape now {master.shape}\")\n",
    "    del part\n",
    "    gc.collect()\n",
    "\n",
    "# ordenar por data e por nome do ticker (opcional)\n",
    "master = master.sort_index()\n",
    "master = master.reindex(columns=sorted(master.columns, key=str))\n",
    "\n",
    "# checar limite de colunas do Excel (DATE + tickers)\n",
    "total_cols_for_excel = master.shape[1] + 1\n",
    "if total_cols_for_excel > EXCEL_MAX_COLS:\n",
    "    raise RuntimeError(\n",
    "        f\"Too many columns for Excel: {total_cols_for_excel} > {EXCEL_MAX_COLS}. \"\n",
    "        f\"Reduza colunas ou salve em Parquet/CSV.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"\\nFinal shape: {master.shape[0]} rows x {master.shape[1]} ticker columns \"\n",
    "    f\"(+ DATE = {total_cols_for_excel} total Excel columns)\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# SALVAR COM OPENPYXL (WRITE-ONLY STREAMING)\n",
    "# =========================\n",
    "wb = Workbook(write_only=True)\n",
    "ws = wb.create_sheet(\"dados\")\n",
    "\n",
    "# remover a sheet padrão criada pelo Workbook()\n",
    "try:\n",
    "    wb.remove(wb[\"Sheet\"])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# cabeçalho\n",
    "header = [\"DATE\"] + master.columns.tolist()\n",
    "ws.append(header)\n",
    "\n",
    "# função rápida para converter NaN -> None (openpyxl não grava NaN)\n",
    "def nan_to_none_row(row_tuple):\n",
    "    arr = np.array(row_tuple, dtype=object)\n",
    "    # pandas.isna lida com floats e None\n",
    "    mask = pd.isna(arr)\n",
    "    if mask.any():\n",
    "        arr[mask] = None\n",
    "    return arr.tolist()\n",
    "\n",
    "# escrever linhas\n",
    "for k, (dt, row) in enumerate(zip(master.index.to_pydatetime(),\n",
    "                                  master.itertuples(index=False, name=None)), start=1):\n",
    "    ws.append([dt] + nan_to_none_row(row))\n",
    "    if k % PRINT_EVERY_ROWS == 0:\n",
    "        print(f\"  -> wrote {k} rows to Excel...\")\n",
    "\n",
    "wb.save(OUTFILE)\n",
    "print(f\"✅ Saved: {OUTFILE}\")\n",
    "\n",
    "# =========================\n",
    "# ALTERNATIVA SIMPLES (se memória permitir):\n",
    "# master.to_excel(OUTFILE, index_label='DATE')\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sandboxed)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
